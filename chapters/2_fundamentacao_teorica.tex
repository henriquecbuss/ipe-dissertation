\phantomsection

\chapter{Fundamentação Teórica}
\phantomsection

Neste capítulo, discutiremos a fundamentação teórica necessária para compreender
o trabalho. Começaremos discutindo APIs REST na \autoref{sec:apis}, que são o
objetivo inicial para programas escritos em Ipe. Depois, na \autoref{sec:analise_codigo_fonte},
discutiremos o processo de compilação.

\section{APIs e APIs REST}\label{sec:apis}

Uma API (\textit{Application Programming Interface}) é um termo usado para representar
a interface de comunicação entre programas de computador. Por exemplo, a coleção
de métodos públicos de uma classe em Java pode ser chamada de API, pois é o
meio de comunicação entre o resto do programa e a classe.

Mais comumente, uma API é a face de um serviço web, ativamente escutando por
requisições HTTP e as respondendo \cite{restapirulebook}, de forma que clientes
(páginas web, aplicativos celulares) possam se comunicar com o serviço.

REST (\textit{Representational State Transfer}) é um estilo de arquitetura para
modelar APIs, de forma que diferentes APIs tenham um formato padrão. Geralmente,
APIs usam o formato JSON (\textit{JavaScript Object Notation}) para representar
dados.

O foco inicial de Ipe é desenvolver APIs REST, ou seja, aplicações escritas em  Ipe devem ser capazes
de escutar por requisições HTTP e respondê-las, usando o formato JSON, seguindo o padrão REST.

\section{Análise de código fonte}\label{sec:analise_codigo_fonte}

Ipe será uma linguagem de programação de alto nível. Isso significa que, em algum
momento, código Ipe (linguagem fonte), deve ser transformado para algo que
computadores entendam (linguagem alvo). Esse processo se chama compilação, e é
feito por um programa chamado compilador \cite{dragonbook}. No caso deste trabalho, a linguagem
alvo é Javascript, por ser uma linguagem com ampla adoção no mercado \cite{stackoverflowsurvey}.
Quando a linguagem alvo também é uma linguagem de alto nível, como é o caso de
Javascript, o processo de compilação é chamado de transpilação.

Compiladores geralmente funcionam com uma série de etapas. Primeiramente, o
código fonte é analisado e transformado em uma \textit{AST}
(\textit{Abstract Syntax Tree}), de forma a garantir que o código fonte é válido de
acordo com a definição léxica e sintática da linguagem fonte. Este processo é
chamado de \textit{parsing}, e discutiremos ele na \autoref{sec:parsing}. Em seguida, a \textit{AST}
é analisada para fazer a checagem de tipos (ou \textit{type checking}, em inglês), o que faz parte
da análise semântica, discutida na \autoref{sec:semantic}. Depois disso,
opcionalmente, são aplicadas transformações no código. Por exemplo, o compilador pode realizar
otimizações, ou remover código não utilizado. Estas transformações são discutidas na
\autoref{sec:transformations}. Por fim, a AST e o código fonte são usados para produzir código na
linguagem alvo. A geração de código da linguagem alvo é discutida na \autoref{sec:code_generation}.
\cite{dragonbook}

A análise de código fonte é a primeira etapa do processo de compilação, e se
baseia na \textit{gramática} da linguagem. A gramática de uma linguagem é uma lista
de regras que dita o que é válido na linguagem. Em qualquer uma das etapas, se o
compilador encontrar um erro, ele deve abortar o processo de compilação e emitir
uma mensagem de erro. A gramática de Ipe é explicada no \autoref{chapter:specification},
e está disponível em sua íntegra no \autoref{apendix:ebnf-syntax}.

A análise de código fonte geralmente é dividida em três etapas, descritas a seguir.

\subsection{Parsing}\label{sec:parsing}

O \textit{parsing} é a primeira etapa da análise de código fonte. Nesta etapa,
transformamos código em linguagem fonte para uma estrutura que o compilador
entenda. Normalmente, o código é representado em formato de árvore. Nesta etapa
da compilação estão as análises léxica e sintática. \cite{dragonbook}

\subsubsection{Análise léxica}

A análise léxica é encarregada de dividir o código fonte em \textit{tokens}. Um
token é uma sequência de caracteres que possui um significado. Por exemplo, a
palavra \textit{if} é um token, assim como o símbolo \textit{+}. A análise léxica
é responsável por identificar esses tokens, e gerar uma lista deles.


\subsubsection{Análise sintática}

A análise sintática recebe como entrada a lista de tokens gerada pela análise
léxica, e verifica se a sequência de tokens é válida de acordo com a gramática da
linguagem. Nesse processo, os tokens são organizados em formato de árvore.

\subsection{Análise semântica}\label{sec:semantic}

A análise semântica usa os dados obtidos pelas etapas anteriores para verificar se
o código fonte é válido de acordo com a semântica da linguagem. Por exemplo, é
nessa etapa em que o compilador realiza a checagem de tipos, e verifica se todas
as variáveis usadas no programa foram declaradas.


\subsection{Transformações de código}\label{sec:transformations}

A análise de código fonte gera dados sobre o programa fonte (tabelas e árvores que
representam o programa). Durante o processo de compilação, é possível realizar
transformações no código fonte, de forma a otimizar o programa, ou remover código
não utilizado. Por simplicidade, Ipe não fará nenhuma otimização.


\subsection{Geração de código}\label{sec:code_generation}

A etapa de geração de código é a etapa final do processo de compilação. Nessa
etapa, o compilador gera o código alvo a partir dos dados gerados pelas etapas
anteriores. No caso de Ipe, o código alvo é Javascript. Ou seja, vamos usar os dados
obtidos nas etapas anteriores para traduzir o código fonte (escrito em Ipe) para
código Javascript. Para isso, mapeamos cada instrução da linguagem Ipe para uma
instrução equivalente em Javascript.

Como o objetivo da linguagem é ser uma linguagem de programação para \textit{backend},
geraremos código compatível com o ambiente Node.js, por ser a plataforma mais popular
para desenvolvimento de aplicações \textit{backend} em Javascript \cite{stackoverflowsurvey}.
